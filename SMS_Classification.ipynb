{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import chardet\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the encoding style of the input csv file.By mentioning the encoding style in read_csv(), we can avoid the chances of getting an error due to encoding style mismatch \n",
    "with open('spam.csv', 'rb') as rawdata:\n",
    "    encode_style =  chardet.detect(rawdata.read(100000))\n",
    "print(encode_style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('spam.csv',encoding='Windows-1252')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the data information \n",
    "# the last 3 column (Unnamed: 2, Unnamed: 3, Unnamed: 4) has maximum null values.To confirm the same, another checking using isnull() will be done\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there is no missing values in first two column but maximum values are missing in the last 3 columns. So, we will drop these last 3 columns \n",
    "data.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping the last 3 columns\n",
    "data.drop(['Unnamed: 2','Unnamed: 3','Unnamed: 4'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To increase the readbility , lets change the column name of first two column (v1 and v2)\n",
    "data.columns = ['Label','Message']\n",
    "#checking the data \n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To understand the details information of these two column. \n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a new column to understand the charactertistics of two type message label \n",
    "data['Message_length'] = data['Message'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the distribution of message size\n",
    "data.groupby('Label')['Message_length'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean length of spam messages are larger than the mean length of the ham messages. Usually, the length of the spam messages are larger in length of the non-spam messages. To verify it again,we will check the the distribution plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_message = data['Message_length'].hist(bins=100,by=data['Label'],figsize=(10,6))\n",
    "dist_message[0].set_xlabel(\"Message Length\")\n",
    "dist_message[0].set_ylabel(\"Freequency\")\n",
    "dist_message[1].set_xlabel(\"Message Length\")\n",
    "dist_message[1].set_ylabel(\"Freequency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this basic EDA it is clear that the spam messages are larger in length. In message label = ham distrbution, there is a message whose length is much higher than the other messages in ham group. Now, it is difficult to get the actual length of this long message from this plot. From the output of our previous data.groupby('Label')['Message_length'].describe(), we can see that the max length is 910. Now, we can also check which message is this in ham group.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To find out the message which has a length of 910 \n",
    "data[data['Message_length'] == 910]['Message'].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_clean(message):\n",
    "    \n",
    "    # removing all punctuation\n",
    "    nopunc = [letter for letter in message if letter not in string.punctuation]\n",
    "    punc_filtered = \"\".join(nopunc)\n",
    "    # removing all stopwords\n",
    "    return [words for words in punc_filtered.split(\" \") if words.lower() not in stopwords.words('english')]\n",
    "    # returning the words as list "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example,we can see how this text_clean() works on messages :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Message'].apply(text_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before performing Vectorization, we will divide the dataset into training and test set to avoid Data leakage. Once these partitions are done,  each of these sets(training and test set) will be converted into vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we are using only the 'Message' column to perform the classfication and we are taking 70% of dataset as training data and the remainder 30% as test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_Train,x_Test,y_Train,y_Test = train_test_split(data['Message'],data['Label'],test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bow = CountVectorizer(analyzer=text_clean).fit(x_Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total number of vocab words\n",
    "print(len(train_bow.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to see the entire vocabulary\n",
    "#train_bow.vocabulary_   # execute this command to see the entire vocabulary and the index position of each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transforming the entire training data set messages:\n",
    "train_matrix = train_bow.transform(x_Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Shape of Sparse Matrix: ', train_matrix.shape)\n",
    "print('Amount of Non-Zero occurences: ', train_matrix.nnz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To assign a weight to each word of the vocabulary, we will use TF-IDF. The words which has higher freequency will be assigned a lower weighatge and the words which are rare and has lower freequency will be assigned a higher weighatge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tf_idf_train = TfidfTransformer().fit(train_matrix)\n",
    "messages_tf_idf_train = tf_idf_train.transform(train_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To confirm that the word with higher freequency has given a lower weightage than the word with lower freequency, we can consider two word from the entire document- 'want' and 'come'(more freequent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf_idf_train.idf_[train_bow.vocabulary_['want']])\n",
    "print(tf_idf_train.idf_[train_bow.vocabulary_['come']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as the word 'come' is more frequent than 'want' in the entire dataframe, it receives lower weightage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_matrix = train_bow.transform(x_Test)\n",
    "tf_idf_test = TfidfTransformer().fit(test_matrix)\n",
    "messages_tf_idf_test = tf_idf_test.transform(test_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here two scikit-learn models will be used: Naive Bayes and KNN. The accuracy of these models will be compared\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = MultinomialNB()\n",
    "nb.fit(messages_tf_idf_train,y_Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = nb.predict(messages_tf_idf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report,accuracy_score\n",
    "print(\"Classification report is: \")\n",
    "print(classification_report(y_Test,y_pred))\n",
    "print(\"Accuracy Score of Naive Bayes model is: \")\n",
    "print(round(accuracy_score(y_Test,y_pred),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GridSearchCV can be used to identify the best value of k for KNN model. But instead of using GridSearchCv, we can also estimate the best value of K in the following way:\n",
    "\n",
    "assuming that the best value of K lies in between 1 to 40. we are creating an instance of KNN here and trying to mesaure the prediction error made by that instance of KNN.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_rate = []\n",
    "for i in range(1,40):\n",
    "    knn = KNeighborsClassifier(n_neighbors=i)\n",
    "    knn.fit(messages_tf_idf_train,y_Train)\n",
    "    y_pred_elbow = knn.predict(messages_tf_idf_test)\n",
    "    error = np.mean((y_Test != y_pred_elbow))\n",
    "    error_rate.append(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(range(1,40),error_rate,linestyle='--',marker='o',markersize=8,markerfacecolor='red')\n",
    "plt.title('Error Rate vs. K Value')\n",
    "plt.xlabel('K')\n",
    "plt.ylabel('Error Rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above plot, it is clear that the error rate is increaseing after K=5. The minimum error that we can get in this task is for k=3 or k=5. The error rate for k=4 will be higher than k=3 or k=5. We will check the performance for these two k value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#k=3\n",
    "knn_3 = KNeighborsClassifier(n_neighbors=3)\n",
    "knn_3.fit(messages_tf_idf_train,y_Train)\n",
    "y_pred = knn_3.predict(messages_tf_idf_test)\n",
    "print(\"Classification report is: \")\n",
    "print(classification_report(y_Test,y_pred))\n",
    "print(\"Accuracy Score of KNN(K=3): \")\n",
    "print(round(accuracy_score(y_Test,y_pred),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#K=4 \n",
    "\n",
    "knn_4 = KNeighborsClassifier(n_neighbors=4)\n",
    "knn_4.fit(messages_tf_idf_train,y_Train)\n",
    "y_pred = knn_4.predict(messages_tf_idf_test)\n",
    "print(\"Classification report is: \")\n",
    "print(classification_report(y_Test,y_pred))\n",
    "print(\"Accuracy Score of KNN(K=4): \")\n",
    "print(round(accuracy_score(y_Test,y_pred),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#K=5\n",
    "knn_5 = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_5.fit(messages_tf_idf_train,y_Train)\n",
    "y_pred = knn_5.predict(messages_tf_idf_test)\n",
    "print(\"Classification report is: \")\n",
    "print(classification_report(y_Test,y_pred))\n",
    "print(\"Accuracy Score of KNN(K=4): \")\n",
    "print(round(accuracy_score(y_Test,y_pred),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy score of K=4 is lower than the K=3 or K=5 ( as it is visible already in the error-rate vs K plot). Between K=3 and K=5, K=3 will be a good choice interms of model accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
